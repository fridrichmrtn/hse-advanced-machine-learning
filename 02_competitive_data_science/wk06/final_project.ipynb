{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Martin Fridrich, 03/2022 \n",
    "\n",
    "# Final project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and initial transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from itertools import product\n",
    "import re\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/final_project/\"\n",
    "# sales\n",
    "sales_train = pd.read_csv(DATA_DIR+\"sales_train.csv\")\n",
    "sales_test = pd.read_csv(DATA_DIR+\"test.csv\")\n",
    "# addional info\n",
    "items = pd.read_csv(DATA_DIR+\"items.csv\")\n",
    "item_categories = pd.read_csv(DATA_DIR+\"item_categories.csv\")\n",
    "shops = pd.read_csv(DATA_DIR+\"shops.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downcast dataframe\n",
    "def optimize_numeric_dtypes(df):\n",
    "    import pandas as pd\n",
    "    float_cols = df.select_dtypes(\"float\").columns\n",
    "    int_cols = df.select_dtypes(\"integer\").columns\n",
    "    df[float_cols] = df[float_cols].\\\n",
    "        apply(pd.to_numeric, downcast=\"float\")\n",
    "    df[int_cols] = df[int_cols].\\\n",
    "        apply(pd.to_numeric, downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "def optimize_categories(df):\n",
    "    import pandas as pd\n",
    "    cat_cols = df.select_dtypes(\"object\").columns\n",
    "    df[cat_cols] = df[cat_cols].\\\n",
    "        apply(lambda x: x.astype(\"category\").cat.codes)\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate shops\n",
    "shop_duplicates_map = {0: 57, 1: 58, 11: 10, 40: 39}\n",
    "sales_train[\"shop_id\"] = sales_train[\"shop_id\"].replace(shop_duplicates_map)\n",
    "# datetime\n",
    "sales_train[\"date\"] = pd.to_datetime(sales_train[\"date\"], format=\"%d.%m.%Y\")\n",
    "# filtering on test set\n",
    "sales_train = sales_train.loc[sales_train.item_id.isin(sales_test[\"item_id\"].unique()), :]\n",
    "# drop training items with extreme or negative prices or sales counts\n",
    "sales_train = sales_train[((sales_train[\"item_price\"] > 0) & (sales_train[\"item_price\"] < 50000)) &\n",
    "    ((sales_train[\"item_cnt_day\"] > 0) & (sales_train[\"item_cnt_day\"] < 1000))]\n",
    "# test-based cartesian product & inds\n",
    "sales_index = pd.DataFrame(product(sales_test.item_id.unique(), sales_test.shop_id.unique(),\n",
    "    range(35)),columns = [\"item_id\", \"shop_id\", \"date_block_num\"])\n",
    "sales_index = optimize_numeric_dtypes(sales_index)\n",
    "# sales table\n",
    "sales_train[\"revenue\"] = sales_train[\"item_price\"]*sales_train[\"item_cnt_day\"]\n",
    "sales = sales_train.groupby([\"shop_id\", \"item_id\", \"date_block_num\"], as_index=False).agg(\n",
    "    total_sold=(\"item_cnt_day\", sum),  sum_revenue=(\"revenue\",sum),\n",
    "    n_transactions = (\"item_cnt_day\",pd.Series.count))\\\n",
    "        .sort_values(\"date_block_num\")\n",
    "sales = sales_index.merge(sales, how=\"left\",\n",
    "    on=[\"shop_id\", \"item_id\", \"date_block_num\"])\n",
    "sales[\"date_block_num\"].fillna(34, inplace=True)\n",
    "sales.loc[sales.date_block_num<34, [\"total_sold\", \"sum_revenue\", \"n_transactions\"]]=\\\n",
    "    sales.loc[sales.date_block_num<34, [\"total_sold\", \"sum_revenue\", \"n_transactions\"]].fillna(0)\n",
    "sales = optimize_numeric_dtypes(sales)\n",
    "del sales_train, sales_test, sales_index; gc.collect()\n",
    "sales.head(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item groups based on adjacency and similarity\n",
    "items.item_name = items.item_name.apply(lambda x: re.sub(\"[^A-ZА-Яa-zа-я0-9 ]\",\"\", x))\n",
    "items.item_name = items.item_name.apply(lambda x: re.sub(\"\\\\s+\",\" \", x))\n",
    "item_names = pd.concat([items.item_id, items.item_name,\n",
    "    items.item_id.shift().fillna(0), items.item_name.shift().fillna(\"\")], axis=1)\n",
    "item_names.columns = [\"item_id\", \"item_name\", \"previous_item_id\", \"previous_item_name\"]\n",
    "item_names[\"similarity\"] = item_names.apply(\\\n",
    "     lambda x: fuzz.ratio(x[\"item_name\"],x[\"previous_item_name\"]), axis=1)\n",
    "item_names[\"group_start\"] = item_names[\"similarity\"] <= 75\n",
    "item_group_map = item_names.loc[item_names.group_start, [\"item_id\"]]\n",
    "item_group_map.columns = [\"group_start\"]\n",
    "item_group_map[\"group_end\"] = item_group_map[\"group_start\"].shift(-1).\\\n",
    "    fillna(item_group_map[\"group_start\"].max()+1).astype(\"int\")\n",
    "item_group_map[\"item_sim_id\"] = list(range(len(item_group_map)))\n",
    "# remap\n",
    "items = items.merge(item_group_map, how=\"left\",\n",
    "    left_on=[\"item_id\"], right_on=[\"group_start\"])\n",
    "items = items.sort_values(\"item_id\")\n",
    "items[\"item_sim_id\"] = items[\"item_sim_id\"].fillna(method=\"ffill\").astype(\"int\")\n",
    "del item_names, item_group_map; gc.collect()\n",
    "items = items[[\"item_id\",\"item_name\", \"item_category_id\", \"item_sim_id\"]]\n",
    "# name length\n",
    "items[\"item_name_len\"] = items.item_name.apply(len).astype(\"int16\")\n",
    "items.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories\n",
    "cat_splits = item_categories.item_category_name.apply(lambda x: x.split(\" - \"))\n",
    "item_categories = pd.concat([item_categories,\n",
    "    pd.DataFrame([c if len(c)>1 else [c[0],\"\"] for c in cat_splits],\n",
    "        columns=[\"parent_cat\",\"child_cat\"])], axis=1)\n",
    "items = items.merge(item_categories, how=\"inner\")\n",
    "# return categorical encoding\n",
    "items = optimize_categories(items[[\"item_id\", \"item_category_id\", \"item_sim_id\",\n",
    "    \"item_name_len\", \"parent_cat\", \"child_cat\"]])\n",
    "# add categories\n",
    "sales = sales.merge(items, how=\"inner\", on=\"item_id\")\n",
    "del item_categories, items; gc.collect()\n",
    "sales.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date features\n",
    "def working_days_count(from_date, to_date=None):\n",
    "    import pandas as pd\n",
    "    if to_date is None:\n",
    "        to_date=from_date+pd.DateOffset(months=1)\n",
    "    temp_date=from_date\n",
    "    cnt=0\n",
    "    while temp_date<to_date:\n",
    "        if temp_date.weekday()<5:\n",
    "            cnt+=1\n",
    "        temp_date+=pd.DateOffset(days=1)\n",
    "    return ((to_date-from_date).days, cnt, (to_date-from_date).days-cnt)\n",
    "dates = pd.DataFrame(sales.date_block_num.unique(), columns=[\"date_block_num\"])\n",
    "dates[\"date\"] = dates[\"date_block_num\"].apply(lambda x: pd.to_datetime(\"01/01/2013\")+\\\n",
    "    pd.DateOffset(months=x))\n",
    "dates[\"year\"] = dates.date.dt.year\n",
    "dates[\"month\"] = dates.date.dt.month\n",
    "dates = optimize_numeric_dtypes(pd.concat([dates, pd.DataFrame.from_records(\n",
    "        dates[\"date\"].apply(lambda x: working_days_count(x)),\n",
    "    columns=[\"total_days\", \"working_days\", \"weekend_days\"])], axis=1))\n",
    "sales = sales.merge(dates[[\"date_block_num\",\"year\",\"month\",\"total_days\", \"working_days\",\n",
    "    \"weekend_days\"]], how=\"inner\", on=\"date_block_num\")\n",
    "del dates; gc.collect()\n",
    "sales.tail(3)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pricing overall and inside cat\n",
    "item_pricing = sales.groupby([\"date_block_num\",\"item_id\",\"item_category_id\"], as_index=False).\\\n",
    "    agg(total_sold=(\"total_sold\", sum),  sum_revenue=(\"sum_revenue\",sum))\n",
    "train_ind = item_pricing.date_block_num<34\n",
    "item_pricing.loc[train_ind,\"average_price\"] = item_pricing[\"sum_revenue\"]/item_pricing[\"total_sold\"]\n",
    "item_pricing.loc[train_ind,\"average_price\"]  = item_pricing.groupby([\"item_id\"])[\"average_price\"].\\\n",
    "    apply(lambda x: x.fillna(method=\"ffill\").fillna(method=\"bfill\"))\n",
    "# add price changes\n",
    "item_pricing.loc[train_ind,\"price_change\"] = item_pricing.groupby([\"item_id\"])[\"average_price\"].\\\n",
    "    apply(lambda x: x/x.shift()).fillna(-1)\n",
    "# add position within the category\n",
    "item_pricing.loc[train_ind,\"price_perc\"] = item_pricing.groupby([\"item_category_id\", \"date_block_num\"])\\\n",
    "    [\"average_price\"].apply(lambda x: x.rank(pct=True)).fillna(-1)\n",
    "item_pricing = optimize_numeric_dtypes(item_pricing[[\"date_block_num\", \"item_id\", \"price_change\",\n",
    "    \"price_perc\"]])\n",
    "sales = sales.merge(item_pricing, how=\"left\",\n",
    "    on=[\"date_block_num\", \"item_id\"])\n",
    "del item_pricing; gc.collect()\n",
    "sales.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shop location\n",
    "shops[\"city_id\"] = shops.shop_name.apply(lambda x:\\\n",
    "    re.sub(\"[^A-ZА-Яa-zа-я0-9 ]\",\"\", x.split(\" \")[0]))\n",
    "#shops[\"city_code_id\"] = shops.shop_name.apply(lambda x:\\\n",
    "#    re.sub(\"[^A-ZА-Яa-zа-я0-9 ]\",\"\", x.split(\" \")[1]))\n",
    "# keep wrong codes, so city and code are not the same\n",
    "# shops[\"city_code\"][shops.city==\"Якутск\"] = \"ТЦ\"\n",
    "shops = optimize_categories(shops[[\"shop_id\", \"city_id\"]])\n",
    "sales = sales.merge(shops, how=\"inner\", on=\"shop_id\")\n",
    "del shops; gc.collect()\n",
    "sales.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time-series\n",
    "# smoothed demand across various cats\n",
    "train_ind = sales.date_block_num<34\n",
    "# city\n",
    "sales.loc[train_ind, \"city_demand\"] = sales[train_ind].\\\n",
    "    groupby([\"city_id\",\"date_block_num\"])[\"total_sold\"].apply(lambda x:\n",
    "        x.ewm(span=3, min_periods=3).mean()).fillna(0)#.astype(\"float32\")\n",
    "# shop\n",
    "sales.loc[train_ind, \"shop_demand\"] = sales[train_ind].\\\n",
    "    groupby([\"shop_id\",\"date_block_num\"])[\"total_sold\"].apply(lambda x:\n",
    "        x.ewm(span=3, min_periods=3).mean()).fillna(0)#.astype(\"float32\")\n",
    "# categories\n",
    "sales.loc[train_ind, \"cat_demand0\"] = sales[train_ind].\\\n",
    "    groupby([\"parent_cat\",\"date_block_num\"])[\"total_sold\"].apply(lambda x:\n",
    "        x.ewm(span=3, min_periods=3).mean()).fillna(0)#.astype(\"float32\")\n",
    "# categories\n",
    "sales.loc[train_ind, \"cat_demand1\"] = sales[train_ind].\\\n",
    "    groupby([\"child_cat\",\"date_block_num\"])[\"total_sold\"].apply(lambda x:\n",
    "        x.ewm(span=3, min_periods=3).mean()).fillna(0)#.astype(\"float32\")      \n",
    "# categories\n",
    "sales.loc[train_ind, \"cat_demand2\"] = sales[train_ind].\\\n",
    "    groupby([\"item_sim_id\",\"date_block_num\"])[\"total_sold\"].apply(lambda x:\n",
    "        x.ewm(span=3, min_periods=3).mean()).fillna(0)#.astype(\"float32\")        \n",
    "# item\n",
    "sales.loc[train_ind, \"item_demand\"] = sales[train_ind].\\\n",
    "    groupby([\"item_id\",\"date_block_num\"])[\"total_sold\"].apply(lambda x:\n",
    "        x.ewm(span=3, min_periods=3).mean()).fillna(0)#.astype(\"float32\")\n",
    "# item maturity\n",
    "sales[\"item_maturity\"] = (sales[\"date_block_num\"] - sales[[\"item_id\",\"date_block_num\"]].merge(\\\n",
    "    sales[sales.n_transactions>0].groupby(\"item_id\", as_index=False).\\\n",
    "        agg(date_block_num_min=(\"date_block_num\",min)),\n",
    "            how=\"left\")[\"date_block_num_min\"]).clip(-1,34).fillna(-1)\n",
    "del train_ind; gc.collect();\n",
    "sales = optimize_numeric_dtypes(sales)\n",
    "sales.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_steps = [1,2,3,6]\n",
    "lag_fill = {\"total_sold\":0, \"sum_revenue\":0, \"n_transactions\":0, \"price_change\":1, \"price_perc\":-1,\n",
    "    \"city_demand\":0, \"shop_demand\":0, \"cat_demand0\":0,\"cat_demand1\":0,\"cat_demand2\":0, \"item_demand\":0}\n",
    "for l in lag_steps:\n",
    "    for c in lag_fill.keys():\n",
    "        sales.loc[:,\"lag\"+str(l)+\"_\"+c] = sales.groupby([sales.shop_id,sales.item_id])\\\n",
    "            [c].shift(l).fillna(lag_fill[c])\n",
    "sales = sales[sales.date_block_num>25] # just reduce the dataset\n",
    "target = \"total_sold\"\n",
    "col_fil = [c for c in sales.columns if (c not in lag_fill.keys()) or (c ==target)]\n",
    "sales = optimize_numeric_dtypes(sales.loc[:,col_fil])\n",
    "sales.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some artificial feature interactions\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from lightgbm import LGBMRegressor\n",
    "lgbm_feature_imp = LGBMRegressor()\n",
    "features = [c for c in sales.columns if (c not in [target])]\n",
    "lgbm_feature_imp.fit(sales.loc[:,features], sales.loc[:,target])\n",
    "feature_imp = pd.DataFrame.from_records(zip(lgbm_feature_imp.feature_name_,\n",
    "    lgbm_feature_imp.feature_importances_),\n",
    "    columns=[\"feature\",\"imp\"]).sort_values(\"imp\", ascending=False)    \n",
    "# use interactions between 5 more prominent features, as they will occur frequently\n",
    "# on trees' decision path\n",
    "pf = PolynomialFeatures(interaction_only=True, include_bias=False).\\\n",
    "    fit(sales.loc[:,feature_imp.feature[:5]])\n",
    "feature_int = pd.DataFrame(pf.transform(sales.loc[:,feature_imp.feature[:5]]),\n",
    "    columns=pf.get_feature_names_out(), index=sales.index)\n",
    "feature_int = optimize_numeric_dtypes(feature_int.loc[:,[c for c in feature_int if \" \" in c]])\n",
    "feature_int.columns = [c.replace(\" \", \"_\") for c in feature_int.columns]\n",
    "sales = pd.concat([sales, feature_int], axis=1)\n",
    "del feature_int, lgbm_feature_imp, feature_imp, pf; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer, PolynomialFeatures\n",
    "from category_encoders import TargetEncoder,PolynomialEncoder,CatBoostEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression,ElasticNetCV,ElasticNet,BayesianRidge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage filters\n",
    "b_ind = (sales.year==2015) & (sales.month==10) # 2nd stage\n",
    "c_ind = (sales.year==2015) & (sales.month==9)\n",
    "test_ind = (sales.year==2015) & (sales.month==11) # prediction\n",
    "a_ind = ~b_ind & ~c_ind & ~test_ind # 1st stage\n",
    "\n",
    "# col filters for mean-encoding\n",
    "target = \"total_sold\"\n",
    "features = [c for c in sales.columns if (c not in [target])]\n",
    "cat_cols = [\"shop_id\", \"item_id\", \"date_block_num\", \"item_name_len\",\n",
    "    \"item_category_id\", \"item_sim_id\", \"parent_cat\", \"child_cat\",\n",
    "    \"year\", \"month\", \"total_days\", \"working_days\", \"weekend_days\",\n",
    "    \"city_id\"]\n",
    "num_cols = [f for f in sales[features].columns if (f not in cat_cols) & (f not in [target])]\n",
    "# clippin\n",
    "sales[target] = sales[target].clip(0,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First stage models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catboost on raw data\n",
    "cat_model = CatBoostRegressor(cat_features=cat_cols,\n",
    "    verbose=100)\n",
    "cat_model.fit(sales.loc[a_ind,features], sales.loc[a_ind,target])\n",
    "\n",
    "# pushout preds\n",
    "cat_b_feature = cat_model.predict(sales.loc[b_ind,features]).clip(0,20)\n",
    "cat_c_feature = cat_model.predict(sales.loc[c_ind,features]).clip(0,20)\n",
    "cat_test_feature = cat_model.predict(sales.loc[test_ind,features]).clip(0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightgbm raw\n",
    "lgbm_model = lgbm_model = Pipeline(steps=[(\"ct\",ColumnTransformer(transformers=[\n",
    "        (\"se\", StandardScaler(), num_cols),\n",
    "        (\"te\", TargetEncoder(cols=cat_cols, drop_invariant=True,\n",
    "            min_samples_leaf=5*10**4, smoothing=10**3), cat_cols)])),\n",
    "    (\"lgbm\", LGBMRegressor(n_estimators=500, learning_rate=.01,\n",
    "        max_depth=4, subsample=.5, num_leaves = 1024))])\n",
    "lgbm_model.fit(sales.loc[a_ind,features], sales.loc[a_ind,target])\n",
    "\n",
    "# pushout preds\n",
    "lgbm_b_feature = lgbm_model.predict(sales.loc[b_ind,features]).clip(0,20)\n",
    "lgbm_c_feature = lgbm_model.predict(sales.loc[c_ind,features]).clip(0,20)\n",
    "lgbm_test_feature = lgbm_model.predict(sales.loc[test_ind,features]).clip(0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightgbm pca\n",
    "lgbm_pca_model = lgbm_model = Pipeline(steps=[(\"ct\",ColumnTransformer(transformers=[\n",
    "        (\"se\", StandardScaler(), num_cols),\n",
    "        (\"te\", TargetEncoder(cols=cat_cols, drop_invariant=True,\n",
    "            min_samples_leaf=50000, smoothing=1000), cat_cols)])),\n",
    "    (\"qe\", QuantileTransformer()),\n",
    "    (\"pca\", IncrementalPCA(n_components=10)),\n",
    "    (\"lgbm\", LGBMRegressor())])\n",
    "lgbm_pca_model.fit(sales.loc[a_ind,features], sales.loc[a_ind,target])\n",
    "\n",
    "# pushout preds\n",
    "lgbm_pca_b_feature = lgbm_pca_model.predict(sales.loc[b_ind,features]).clip(0,20)\n",
    "lgbm_pca_c_feature = lgbm_pca_model.predict(sales.loc[c_ind,features]).clip(0,20)\n",
    "lgbm_pca_test_feature = lgbm_pca_model.predict(sales.loc[test_ind,features]).clip(0,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second stage model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_features = np.stack([cat_b_feature, lgbm_b_feature, lgbm_pca_b_feature], axis=1)\n",
    "c_features = np.stack([cat_c_feature, lgbm_c_feature, lgbm_pca_c_feature], axis=1)\n",
    "test_features = np.stack([cat_test_feature, lgbm_test_feature, lgbm_pca_test_feature], axis=1)\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(b_features, sales.loc[b_ind,target])\n",
    "\n",
    "train_rsme = np.mean((lr_model.predict(b_features).clip(0,20)-sales.loc[b_ind,target])**2)**(1/2)\n",
    "val_rsme = np.mean((lr_model.predict(c_features).clip(0,20)-sales.loc[c_ind,target])**2)**(1/2)\n",
    "print(\"Stacked pipeline> RMSE on train set: {}; on val set: {}\".format(train_rsme, val_rsme))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mapping = pd.read_csv(DATA_DIR+\"test.csv\")\n",
    "test_predictions = sales.loc[test_ind,[\"item_id\", \"shop_id\"]]\n",
    "test_predictions[\"item_cnt_month\"] = lr_model.predict(test_features).clip(0,20)\n",
    "test_predictions = test_mapping.merge(test_predictions, on=[\"item_id\", \"shop_id\"],how=\"left\").fillna(0)\n",
    "test_predictions[[\"ID\", \"item_cnt_month\"]].to_csv(\"../data/submissions/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c competitive-data-science-predict-future-sales -f ../data/submissions/submission.csv -m \"Meh, but...\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe955f705e5e305f9f1f02456b3a54982590d8b1fc3c3fbafb7ab752acef3d16"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('py-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
